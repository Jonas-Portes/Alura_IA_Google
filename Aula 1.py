# -*- coding: utf-8 -*-
"""Gemini with Alura

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16WKbp7JFAfx8mK-YL69ZKoIbNGYrh9EA

# Aula 1
"""

!pip install -q --upgrade langchain langchain-gemini google-generativeai langchain-google-genai

from google.colab import userdata
from langchain_google_genai import ChatGoogleGenerativeAI


google_API = userdata.get('key-gemini')

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0,
    api_key= google_API
)

DivaResp = llm.invoke("Quantos anos vc tem?")
print(DivaResp.content)

TRIAGEM_PROMPT = (
    "Você é um triador de Service Desk para políticas internas da empresa Carraro Desenvolvimento. "
    "Dada a mensagem do usuário, retorne SOMENTE um JSON com:\n"
    "{\n"
    '  "decisao": "AUTO_RESOLVER" | "PEDIR_INFO" | "ABRIR_CHAMADO",\n'
    '  "urgencia": "BAIXA" | "MEDIA" | "ALTA",\n'
    '  "campos_faltantes": ["..."]\n'
    "}\n"
    "Regras:\n"
    '- **AUTO_RESOLVER**: Perguntas claras sobre regras ou procedimentos descritos nas políticas (Ex: "Posso reembolsar a internet do meu home office?", "Como funciona a política de alimentação em viagens?").\n'
    '- **PEDIR_INFO**: Mensagens vagas ou que faltam informações para identificar o tema ou contexto (Ex: "Preciso de ajuda com uma política", "Tenho uma dúvida geral").\n'
    '- **ABRIR_CHAMADO**: Pedidos de exceção, liberação, aprovação ou acesso especial, ou quando o usuário explicitamente pede para abrir um chamado (Ex: "Quero exceção para trabalhar 5 dias remoto.", "Solicito liberação para anexos externos.", "Por favor, abra um chamado para o RH.").'
    "Analise a mensagem e decida a ação mais apropriada."
)

from pydantic import BaseModel, Field
from typing import Literal, List, Dict

class TriageOut(BaseModel):
  decisao:Literal["AUTO_RESOLVER" , "PEDIR_INFO" , "ABRIR_CHAMADO"]
  urgencia:Literal["BAIXA" , "MEDIA" , "ALTA"]
  campo_faltantes: List[str] = Field(default_factory=list)

llm_triagem = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0,
    api_key= google_API
)

from langchain_core.messages import SystemMessage, HumanMessage

triagem_chain = llm_triagem.with_structured_output(TriageOut)

def triagem(message: str) -> Dict:
  saida: TriageOut = triagem_chain.invoke((
      SystemMessage(content=TRIAGEM_PROMPT),
      HumanMessage(content=message)
  ))
  return saida.model_dump()

testes = ["Posso reembolsar a internet?",
          "Quero mais 5 dias de trabalho remoto. Como faço?",
          "Posso reembolsar cursos ou treinamentos da Alura?",
          "Quantas capivaras tem no Rio Pinheiros?",
          "Quantos dias tenho de ferias?"]

for i in testes:
  print(f"Pergunta: {i}\n Resposta: {triagem(i)}")

DivaResp = llm.invoke("Quantos anos vc tem?")
print(DivaResp.content)

"""# Aula 2"""

!pip install -q langchain_community faiss-cpu langchain-text-splitters pymupdf

from os import path
from pathlib import Path
from langchain_community.document_loaders import PyMuPDFLoader

docs = []

for n in Path("/content/").glob("*.pdf"):
  try:
    loader = PyMuPDFLoader(str(n))
    docs.extend(loader.load())
    print(f"Carregado arquivo: {n.name}")
  except Exception as e:
      print(f"Erro ao carregar arquivo: {n.name}: {e}")

print(f"Total de documentos carregados: {len(docs)}")

from langchain_text_splitters import RecursiveCharacterTextSplitter as RCTS

spliiter = RCTS(chunk_size=300, chunk_overlap=30)

chunks = spliiter.split_documents(docs)

for chunk in chunks:
    print(chunk)
    print('\n\n')

from langchain_google_genai import GoogleGenerativeAIEmbeddings

embeddings = GoogleGenerativeAIEmbeddings(
    model="models/gemini-embedding-001",
    google_api_key=google_API
)

from langchain_community.vectorstores import FAISS

vectorstore = FAISS.from_documents(chunks, embeddings)

retiever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={ "score_threshold": 0.3,"k": 4 }
    )

from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain

prompt_rag = ChatPromptTemplate.from_messages([
    (
        "system",
        "Você é um Assistende de Politicas Interas (RH/IT) da empresa Carraro Desenvolvimento."
        "Reponda SOMENTE com base no contexto fornecido."
        "Se não houver base suficiente, responda APENAS 'Não Sei'."
    ),
    (
        "human", "Pergunta: {input}\n\nContexto:\n{context})"
    )

])

document_chain = create_stuff_documents_chain(llm, prompt_rag)

# Formatadores
import re, pathlib

def _clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()

def extrair_trecho(texto: str, query: str, janela: int = 240) -> str:
    txt = _clean_text(texto)
    termos = [t.lower() for t in re.findall(r"\w+", query or "") if len(t) >= 4]
    pos = -1
    for t in termos:
        pos = txt.lower().find(t)
        if pos != -1: break
    if pos == -1: pos = 0
    ini, fim = max(0, pos - janela//2), min(len(txt), pos + janela//2)
    return txt[ini:fim]

def formatar_citacoes(docs_rel: List, query: str) -> List[Dict]:
    cites, seen = [], set()
    for d in docs_rel:
        src = pathlib.Path(d.metadata.get("source","")).name
        page = int(d.metadata.get("page", 0)) + 1
        key = (src, page)
        if key in seen:
            continue
        seen.add(key)
        cites.append({"documento": src, "pagina": page, "trecho": extrair_trecho(d.page_content, query)})
    return cites[:3]

def perguntar_RAG(pergunta: str) -> Dict:
    docs_rel = retiever.invoke(pergunta)
    if not docs_rel:
        return {
            "answer": "Não sei.",
            "citacoes": [],
            "contexto_encontrado": False
        }
    answer = document_chain.invoke({
        "input": pergunta,
        "context": docs_rel
        })

    txt = (answer or "").strip()

    if txt.rstrip(".!?") == "Não sei":
        return {
            "answer": "Não sei.",
            "citacoes": [],
            "contexto_encontrado": False
        }

    return {
        "answer": txt,
        "citacoes": formatar_citacoes(docs_rel,pergunta),
        "contexto_encontrado": True
    }

testes = ["Posso reembolsar a internet?",
          "Quantas capivaras tem no Rio Pinheiros?",
          "Quantos dias tenho de ferias?"]

for i in testes:
    resposta = perguntar_RAG(i)
    print(f"Pergunta: {i}")
    print(f"Resposta: {resposta['answer']}")

    if resposta['contexto_encontrado']:
        print("CITAÇÕES:")
        for c in resposta['citacoes']:
            print(f" - Documento: {c['documento']}, Página: {c['pagina']}")
            print(f"   Trecho: {c['trecho']}")

    print("\n")

"""# Aula 3"""

!pip install -q LangGraph

from typing import TypedDict, Optional

class AgentState(TypedDict, total = False):
    pergunta: str
    triagem: dict
    resposta: Optional[str]
    citacoes: List[dict]
    rag_sucesso: bool
    acao_final: str

def node_triagem(state: AgentState) -> AgentState:
    print("Executando nó de Triagem")
    return {"triagem": triagem(state["pergunta"])}

def node_auto_resolver(state: AgentState) -> AgentState:
    print("Executando Auto Resolver")
    resposta_rag = perguntar_RAG(state["pergunta"])

    update: AgentState = {
        "resposta": resposta_rag["answer"],
        "citacoes": resposta_rag.get("citacoes",[]),
        "rag_sucesso": resposta_rag["contexto_encontrado"]
    }
    if resposta_rag["contexto_encontrado"]:
        update["acao_final"] = "AUTO_RESOLVER"
    return update

def node_pedir_info(state: AgentState) -> AgentState:
    print("Executando Pedir info")

    faltantes = state["triagem"].get("campos_faltantes", [])

    if faltantes:
        detalhe = ",".join(faltantes)
    else:
        detalhe = "Tema e contexto específico"
    return {
        "resposta": f"Por favor, preencha os campos: {detalhe}.",
        "citacoes": [],
        "acao_final": "PEDIR_INFO"
    }

def node_abrir_chamado(state: AgentState) -> AgentState:
    print("Executando abrir chamado")
    triagem = state["triagem"]

    return {
        "resposta": f"Abrindo chamado com urgência {triagem['urgencia']}. Descrição: {state['pergunta'][:140]}",
        "citacoes": [],
        "acao_final": "ABRIR_CHAMADO"
    }

KEYWORDS_TICKETS =["aprovação", "liberação", "exceção", "abrir chamado", "acesso especial", "abrir ticket"]

def decidir_pos_triagem(state: AgentState) -> str:
    print("Decidir pos triagem")
    decisao = state["triagem"]["decisao"]

    if decisao == "AUTO_RESOLVER":
        return "auto_resolver"
    elif decisao == "PEDIR_INFO":
        return "pedir_info"
    elif decisao == "ABRIR_CHAMADO":
        return "abrir_chamado"

def decidir_pos_auto_resolver(state: AgentState) -> str:
    print("Decidir pos auto resolution")

    if state.get("rag_sucesso"):
        print("RAG Sucesso, finalizando fluxo.")
        return "ok"

    state_pergunta = (state["pergunta"] or "").lower()

    if any(k in state_pergunta for k in KEYWORDS_TICKETS):
        print("RAG falhou, mas é possivel abrir um ticket, abrindo ticket.")
        return "abrir_chamado"

    print("RAG falhou, não é possivel abrir um ticket, pedir mais info.")
    return "pedir_info"

from langgraph.graph import StateGraph, START, END

workflow = StateGraph(AgentState)

workflow.add_node("triagem", node_triagem)
workflow.add_node("auto_resolver", node_auto_resolver)
workflow.add_node("pedir_info", node_pedir_info)
workflow.add_node("abrir_chamado", node_abrir_chamado)

workflow.add_edge(START, "triagem")
workflow.add_conditional_edges("triagem", decidir_pos_triagem, {
    "auto_resolver": "auto_resolver",
    "pedir_info": "pedir_info",
    "abrir_chamado": "abrir_chamado"
    })

workflow.add_conditional_edges("auto_resolver", decidir_pos_auto_resolver, {
    "pedir_info": "pedir_info",
    "abrir_chamado": "abrir_chamado",
    "ok": END
    })

workflow.add_edge("pedir_info", END)
workflow.add_edge("abrir_chamado", END)

grafo = workflow.compile()

from IPython.display import display, Image

graph_bytes = grafo.get_graph().draw_mermaid_png()
display(Image(graph_bytes))

testenovo = ["Posso reembolsar a internet?",
            "Quero mais 5 dias de trabalho remoto. Como faço?",
            "Posso reembolsar cursos ou treinamentos da Alura?",
            "Quantas capivaras tem no Rio Pinheiros?",
            "Qual o meu nome?",
            "Eu sei ler?",
            "Como esta o mundo de hoje?",
            "Quantos dias tenho de ferias?"]

import time
for msg_test in testenovo:
    time.sleep(5)
    resposta_final = grafo.invoke({"pergunta": msg_test})

    triag = resposta_final.get("triagem", {})
    print("\n")
    print(f"PERGUNTA: {msg_test}")
    print(f"DECISÃO: {triag.get('decisao')} | URGÊNCIA: {triag.get('urgencia')} | AÇÃO FINAL: {resposta_final.get('acao_final')}")
    print(f"RESPOSTA: {resposta_final.get('resposta')}")
    if resposta_final.get("citacoes"):
        print("CITAÇÕES:")
        for citacao in resposta_final.get("citacoes"):
            print(f" - Documento: {citacao['documento']}, Página: {citacao['pagina']}")
            print(f"   Trecho: {citacao['trecho']}")

    print("\n------------------------------------")

!pip freeze > requirements.txt